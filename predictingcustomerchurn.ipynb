{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Imports\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "# Step 2: Load Data (replace with your churn dataset path)\n",
        "df = pd.read_csv(\"/content/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
        "\n",
        "# Step 3: Define Features & Target\n",
        "X = df.drop(\"Churn\", axis=1)   # Features\n",
        "y = df[\"Churn\"]                # Target\n",
        "\n",
        "# Step 4: Split Train & Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Identify Column Types\n",
        "num_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "cat_features = X.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "# Step 6: Preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_features),    # scale numeric features\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)  # encode categoricals\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Step 7: Create Pipeline (Logistic Regression as example)\n",
        "log_reg_pipeline = Pipeline([\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Step 8: GridSearchCV for Logistic Regression\n",
        "param_grid_logreg = {\n",
        "    \"classifier__C\": [0.1, 1, 10],\n",
        "    \"classifier__solver\": [\"liblinear\", \"lbfgs\"]\n",
        "}\n",
        "\n",
        "grid_logreg = GridSearchCV(log_reg_pipeline, param_grid_logreg, cv=5, scoring=\"accuracy\")\n",
        "grid_logreg.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Logistic Regression Params:\", grid_logreg.best_params_)\n",
        "\n",
        "# Step 9: Test Best Logistic Regression Model\n",
        "y_pred_logreg = grid_logreg.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
        "print(classification_report(y_test, y_pred_logreg))\n",
        "\n",
        "# Step 10: Random Forest Pipeline\n",
        "rf_pipeline = Pipeline([\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Step 11: GridSearchCV for Random Forest\n",
        "param_grid_rf = {\n",
        "    \"classifier__n_estimators\": [50, 100],\n",
        "    \"classifier__max_depth\": [5, 10, None]\n",
        "}\n",
        "\n",
        "grid_rf = GridSearchCV(rf_pipeline, param_grid_rf, cv=5, scoring=\"accuracy\")\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Random Forest Params:\", grid_rf.best_params_)\n",
        "\n",
        "# Step 12: Test Best Random Forest Model\n",
        "y_pred_rf = grid_rf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Step 13: Save Best Model (example: Random Forest)\n",
        "joblib.dump(grid_rf.best_estimator_, \"best_churn_model.pkl\")\n",
        "\n",
        "# Step 14: Load Saved Model and Predict\n",
        "loaded_model = joblib.load(\"best_churn_model.pkl\")\n",
        "sample_pred = loaded_model.predict(X_test[:5])\n",
        "print(\"Predictions on sample:\", sample_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAYiDx8ZhV2s",
        "outputId": "19314b32-ab6d-43ac-cf86-3b4663f40e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Logistic Regression Params: {'classifier__C': 1, 'classifier__solver': 'lbfgs'}\n",
            "Logistic Regression Accuracy: 0.8261178140525195\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.86      0.91      0.88      1036\n",
            "         Yes       0.70      0.60      0.64       373\n",
            "\n",
            "    accuracy                           0.83      1409\n",
            "   macro avg       0.78      0.75      0.76      1409\n",
            "weighted avg       0.82      0.83      0.82      1409\n",
            "\n",
            "Best Random Forest Params: {'classifier__max_depth': None, 'classifier__n_estimators': 100}\n",
            "Random Forest Accuracy: 0.7963094393186657\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.82      0.92      0.87      1036\n",
            "         Yes       0.67      0.46      0.54       373\n",
            "\n",
            "    accuracy                           0.80      1409\n",
            "   macro avg       0.75      0.69      0.71      1409\n",
            "weighted avg       0.78      0.80      0.78      1409\n",
            "\n",
            "Predictions on sample: ['Yes' 'No' 'No' 'Yes' 'No']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Hugging Face Transformers (includes PyTorch)\n",
        "!pip install -q transformers datasets"
      ],
      "metadata": {
        "id": "B0P5HCfBUJ7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Overview**\n",
        "\n",
        "Built an end-to-end ML pipeline for customer churn prediction using the Telco Churn dataset.\n",
        "\n",
        "Focused on making the solution reusable and production-ready.\n",
        "\n",
        "Steps include preprocessing, model training, hyperparameter tuning, evaluation, and model export.\n",
        "\n",
        " **Why Pipeline?**\n",
        "\n",
        "Automates data preprocessing + model training into a single workflow.\n",
        "\n",
        "Ensures preprocessing (scaling, encoding) is always applied correctly to new data.\n",
        "\n",
        "Makes the workflow clean, reusable, and ready for deployment.\n",
        "\n",
        " ***Why Joblib?***\n",
        "\n",
        "Saves the entire trained pipeline (preprocessing + model).\n",
        "\n",
        "Avoids retraining the model every time.\n",
        "\n",
        "Enables quick reloading and direct predictions on new data.\n",
        "\n",
        "***Workflow Steps***\n",
        "\n",
        "Load dataset and separate features/target.\n",
        "\n",
        "Train-test split for evaluation.\n",
        "\n",
        "Preprocess numeric (scaling) and categorical (encoding) features.\n",
        "\n",
        "Build pipelines with Logistic Regression and Random Forest.\n",
        "\n",
        "Use GridSearchCV for hyperparameter tuning.\n",
        "\n",
        "Evaluate both models with accuracy, precision, recall, and f1-score.\n",
        "\n",
        "Export the best pipeline with Joblib.\n",
        "\n",
        "Reload and use the saved model for new predictions.\n",
        "\n",
        " **Results**\n",
        "Logistic Regression\n",
        "\n",
        "Best Params: C = 1, solver = lbfgs\n",
        "\n",
        "Accuracy: ~83%\n",
        "\n",
        "Performs better overall, especially in recall for \"No churn\".\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Best Params: n_estimators = 100, max_depth = None\n",
        "\n",
        "Accuracy: ~80%\n",
        "\n",
        "Slightly weaker performance compared to Logistic Regression on this dataset.\n",
        "\n",
        "Sample Predictions\n",
        "\n",
        "Example output: ['Yes', 'No', 'No', 'Yes', 'No']\n",
        "\n",
        " **Final Notes**\n",
        "\n",
        "Logistic Regression was the stronger model here.\n",
        "\n",
        "The saved .pkl file can be reused in a web app or API without retraining.\n",
        "\n",
        "This project demonstrates ML pipeline construction, tuning, and deployment readiness."
      ],
      "metadata": {
        "id": "2HCpsiroqAGE"
      }
    }
  ]
}